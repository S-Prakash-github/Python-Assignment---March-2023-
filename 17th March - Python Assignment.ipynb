{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035d9728",
   "metadata": {},
   "source": [
    "#### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0ccb7",
   "metadata": {},
   "source": [
    "Missing values refer to the absence of data for one or more variables in a dataset. These missing values may arise due to various reasons, such as incomplete data collection, data entry errors, or data loss during transmission or storage.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can lead to biased or inaccurate results in data analysis and modeling. Ignoring missing values can lead to an incomplete or biased understanding of the relationship between variables, and can also reduce the effectiveness of machine learning models.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "- Decision Trees: Decision trees can handle missing values by splitting the data based on available variables, and creating a separate branch for missing values.\n",
    "\n",
    "- Random Forest: Similar to decision trees, random forests can handle missing values by splitting the data based on available variables and constructing a separate branch for missing values.\n",
    "\n",
    "- K-Nearest Neighbors (KNN): KNN can handle missing values by ignoring the missing values and computing the distances between available data points only.\n",
    "\n",
    "- Gradient Boosting: Gradient boosting algorithms can handle missing values by splitting the data based on available variables and constructing a separate branch for missing values.\n",
    "\n",
    "- Naive Bayes: Naive Bayes algorithms can handle missing values by ignoring the missing values and computing the probabilities based on available data only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f277dd",
   "metadata": {},
   "source": [
    "#### Q2: List down techniques used to handle missing data.  Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bc532",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data in machine learning and data analysis. Here are some common techniques with examples in Python using the pandas library:\n",
    "\n",
    "Deletion: This technique involves deleting rows or columns with missing data. This can be done using the dropna() method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63286482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, None, 8]})\n",
    "\n",
    "# drop rows with any missing values\n",
    "df_drop_rows = df.dropna()\n",
    "print(df_drop_rows)\n",
    "\n",
    "# drop columns with any missing values\n",
    "df_drop_cols = df.dropna(axis=1)\n",
    "print(df_drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76af04d",
   "metadata": {},
   "source": [
    "Imputation: This technique involves filling in missing values with estimated values. This can be done using various methods such as mean, median, mode, or regression. In pandas, we can use the fillna() method to fill in missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bab42",
   "metadata": {},
   "source": [
    "##### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b39747d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B\n",
       "0  1.000000  5.0\n",
       "1  2.000000  6.5\n",
       "2  2.333333  6.5\n",
       "3  4.000000  8.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, None, 8]})\n",
    "df.mean = df.fillna(df.mean())\n",
    "df.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333d35b",
   "metadata": {},
   "source": [
    "##### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1cfc04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B\n",
       "0  1.0  5.0\n",
       "1  2.0  6.5\n",
       "2  2.0  6.5\n",
       "3  4.0  8.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, None, 8]})\n",
    "df_median = df.fillna(df.median())\n",
    "df_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6636d",
   "metadata": {},
   "source": [
    "#### Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "022238fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Sky\n",
       "1    Sky\n",
       "2    Sky\n",
       "3    Sky\n",
       "4    Rat\n",
       "Name: B, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, None, 4,5], 'B': ['Sky', None, None, 'Sky','Rat']})\n",
    "df['B']= df['B'].fillna(df['B'].mode()[0])     \n",
    "df['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59554b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81605c91",
   "metadata": {},
   "source": [
    "Interpolation: This technique involves estimating missing values by interpolating between adjacent values. This can be done using methods such as linear interpolation, spline interpolation, or time series interpolation. In pandas, we can use the interpolate() method to interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b67bf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B\n",
       "0  1.0  5.0\n",
       "1  2.0  6.0\n",
       "2  3.0  7.0\n",
       "3  4.0  8.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, None, 8]})\n",
    "df_interpolate = df.interpolate()\n",
    "df_interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2ab32",
   "metadata": {},
   "source": [
    "#### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847d75c",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the number of observations in one class or category of a binary or multi-class classification problem is significantly smaller than the number of observations in the other class or categories. For example, in a binary classification problem where the task is to predict whether a transaction is fraudulent or not, if the number of fraudulent transactions is much smaller than the number of non-fraudulent transactions, the data is said to be imbalanced.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased or inaccurate model performance. Specifically, a model trained on imbalanced data may tend to favor the majority class and perform poorly on the minority class. This is because the model will be optimized to minimize the overall error, and therefore will focus on correctly classifying the majority class, while ignoring the minority class.\n",
    "\n",
    "In many real-world applications, the minority class is often the one of most interest, as it represents a rare event or a critical outcome. Therefore, it is crucial to address the issue of imbalanced data to ensure that the model can accurately capture the patterns and relationships in both the majority and minority classes.\n",
    "\n",
    "There are several techniques that can be used to handle imbalanced data, including:\n",
    "\n",
    "- Undersampling the majority class\n",
    "- Oversampling the minority class\n",
    "- Synthetic minority oversampling technique (SMOTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7e607",
   "metadata": {},
   "source": [
    "#### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down sampling are require"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d14dd6",
   "metadata": {},
   "source": [
    "In the context of machine learning and signal processing, downsampling and upsampling are techniques used to modify the resolution or sampling rate of a signal or data set.\n",
    "\n",
    "Downsampling refers to the process of reducing the sampling rate of a signal by selecting a subset of the original samples. This is typically done by taking every nth sample from the original signal, where n is an integer greater than 1. Downsampling can be useful for reducing the storage requirements and computational cost of processing large data sets, but it can also result in loss of information or aliasing effects if not done carefully.\n",
    "\n",
    "Upsampling refers to the process of increasing the sampling rate of a signal by adding additional samples between the original samples. This is typically done by interpolation, which involves estimating the values of the new samples based on the existing samples. Upsampling can be useful for improving the resolution of a signal or data set, but it can also result in increased storage requirements and computational cost.\n",
    "\n",
    "Both downsampling and upsampling are often used in conjunction with signal processing techniques such as filtering, feature extraction, and classification to extract useful information from signals or data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cdc5f",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd72b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame ({'feature_1' : np.random.normal(loc=0,size=900,scale = 1 ),\n",
    "'feature_2' : np.random.normal(loc=0,size = 900, scale = 1),\n",
    "'target' : [0]*900})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2daef5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'feature_1' : np.random.normal(loc=0,size=100,scale = 1 ),\n",
    "'feature_2' : np.random.normal(loc=0,size = 100, scale = 1),\n",
    "'target' : [1]*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f603f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([df1,df2]).reset_index (drop =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca5950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    900\n",
       "1    100\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09761bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_minority = df[df['target']==1] #1 is minority\n",
    "df_majority = df[df['target']==0] ##0 is majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57651907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    900\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c5acec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85bed050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority_upsample = resample(df_minority,\n",
    "                        replace=True, ## Sample With replacement\n",
    "                              n_samples=len(df_majority), # to match the majority class)\n",
    "                              random_state=42\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe3ed3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    900\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minority_upsample['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b136254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled = pd.concat([df_minority_upsample , df_majority]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50294202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upsampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a88e29",
   "metadata": {},
   "source": [
    "#### Down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "923389b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loc defines the mean value and scale defines the standard deviation value\n",
    "df1 = pd.DataFrame({\n",
    "    'feature_1' : np.random.normal(loc = 0, scale = 1 , size = 1500) , \n",
    "    'fesatur_2' : np.random.normal(loc = 0 , scale = 1 , size = 1500),\n",
    "    'target' : [1]*1500\n",
    "})\n",
    "df2 =pd.DataFrame ({\n",
    "    'feature_1' : np.random.normal(loc = 0, scale = 1 , size = 500) , \n",
    "    'fesatur_2' : np.random.normal(loc = 0 , scale = 1 , size = 500),\n",
    "    'target' : [0]*500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d69cb3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2]).reset_index (drop = True) ## drop = True removes own index with default index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef93d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority = df[df['target']==0]\n",
    "df_majority = df[df['target']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8af9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority_downsampled = resample(df_majority , \n",
    "                                  replace = True,\n",
    "                                  n_samples =len( df_minority),\n",
    "                                  random_state = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a32eda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled = pd.concat([df_majority_downsampled,df_minority]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb71b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "1    500\n",
      "0    500\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_downsampled.shape)\n",
    "print(df_downsampled['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc186607",
   "metadata": {},
   "source": [
    "#### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856d65b",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning and computer vision to artificially increase the size of a dataset by creating additional training data from existing data. This is typically done by applying various transformations to the existing data, such as flipping, rotating, scaling, or adding noise, to create new data points that are similar but not identical to the original data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique used to address imbalanced data in classification problems. It works by creating synthetic minority class samples by interpolating between existing minority class samples.\n",
    "\n",
    "The SMOTE algorithm involves the following steps:\n",
    "\n",
    "Select a minority class sample x.\n",
    "Find the k nearest neighbors of x in the feature space.\n",
    "Select one of the k neighbors, say y.\n",
    "Generate a new synthetic sample by taking a weighted average of x and y, where the weights are chosen randomly between 0 and 1.\n",
    "Repeat steps 1-4 until the desired number of synthetic samples has been generated.\n",
    "SMOTE is effective at increasing the size of the minority class and improving the performance of machine learning models on imbalanced datasets. It is often used in conjunction with other techniques, such as undersampling the majority class or adjusting class weights, to further improve model performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24b49e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "## creating the dataset with tow features x independent and y dependent\n",
    "X,Y = make_classification(n_samples = 1000 , n_features = 2,n_redundant = 0, n_clusters_per_class = 1,\n",
    "                           weights = [0.90],random_state=1)\n",
    "df1 = pd.DataFrame(X,columns = ['f1','f2'])\n",
    "df2 = pd.DataFrame(Y, columns= ['target'])\n",
    "final_df = pd.concat([df1,df2],axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7cd23693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "609dc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X,Y = oversample.fit_resample(final_df[['f1','f2']],final_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4379672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1788, 2), (1788,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "550dff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(X,columns=['f1','f2'])\n",
    "df2=pd.DataFrame(Y,columns=['target'])\n",
    "oversample_df=pd.concat([df1,df2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "786727d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace065e",
   "metadata": {},
   "source": [
    "#### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2329c3",
   "metadata": {},
   "source": [
    "In statistics, outliers are data points that significantly differ from other data points in a dataset. These are extreme values that lie far away from the other data points and do not fit into the overall pattern or distribution of the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on statistical analyses and machine learning models. Outliers can skew the results of statistical measures such as the mean and standard deviation and can lead to inaccurate conclusions. In machine learning, outliers can cause the model to overfit the data or underperform when making predictions on new data.\n",
    "\n",
    "There are several techniques for handling outliers, including removing them from the dataset, replacing them with a more reasonable value, or transforming the data to reduce the effect of outliers. The best approach depends on the nature of the data, the analysis being performed, and the goals of the study.\n",
    "\n",
    "Handling outliers can lead to more accurate results and better machine learning models, so it is essential to identify and deal with them appropriately.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba432cd",
   "metadata": {},
   "source": [
    "#### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818d8a",
   "metadata": {},
   "source": [
    "When dealing with missing data in customer data analysis, there are several techniques that can be used:\n",
    "\n",
    "- Deletion: This technique involves deleting rows or columns with missing data. If the amount of missing data is small, this method might be appropriate. However, if the missing data is large, this technique can lead to a loss of valuable information.\n",
    "\n",
    "- Imputation: This technique involves filling in missing values with estimated values. This can be done using various methods such as mean, median, mode, or regression. Imputation can help preserve valuable information and minimize the impact of missing data.\n",
    "\n",
    "- Prediction models: This technique involves building a prediction model to estimate missing values. This can be done using machine learning algorithms such as K-Nearest Neighbors or Decision Trees. Prediction models can be more accurate than simple imputation methods and can help preserve valuable information.\n",
    "\n",
    "- Interpolation: This technique involves estimating missing values by interpolating between adjacent values. This can be done using methods such as linear interpolation, spline interpolation, or time series interpolation. Interpolation can help preserve valuable information and can be particularly useful in time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23012dd3",
   "metadata": {},
   "source": [
    "#### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcb623",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if missing data is missing at \n",
    "random or if there is a pattern to the missing data:\n",
    "\n",
    "- Visual inspection: One simple approach is to create a visualization of the missing data. This can be done using a heatmap or a bar chart to show the distribution of missing values across variables. If the missing data is random, we would expect to see a relatively uniform distribution of missing values across variables. However, if there is a pattern to the missing data, we would see clusters of missing values in certain variables or combinations of variables.\n",
    "\n",
    "- Statistical tests: Another approach is to use statistical tests to determine if the missing data is missing at random. One commonly used test is Little's MCAR test, which tests the null hypothesis that the missing data is missing completely at random (MCAR). If the p-value from the test is high, we fail to reject the null hypothesis and conclude that the missing data is MCAR.\n",
    "\n",
    "- Imputation and analysis: Imputing the missing data using different methods and analyzing the resulting dataset can also provide insights into the nature of the missing data. For example, if the imputed data results in a significant change in the distribution of a variable or in the results of the analysis, this may indicate that the missing data is not missing at random.\n",
    "\n",
    "- Domain knowledge: Finally, domain knowledge can be used to determine if the missing data is likely to be missing at random or if there is a pattern to the missing data. For example, if missing data occurs only in certain groups of observations, this may indicate that the missing data is not missing at random.\n",
    "\n",
    "- In general, a combination of these strategies may be used to determine if the missing data is missing at random or if there is a pattern to the missing data. It is important to carefully consider the nature of the data and the research question when selecting an approach for handling missing data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608c14a",
   "metadata": {},
   "source": [
    "#### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7f096",
   "metadata": {},
   "source": [
    "- SMOTE (Synthetic Minority Over-sampling Technique): As mentioned earlier, SMOTE can also be used to balance the dataset by generating synthetic samples from the minority class. However, in this case, we need to use SMOTE to generate new samples from the minority class until it is balanced with the majority class.\n",
    "- Random under-sampling: This technique involves randomly removing samples from the majority class until the dataset is balanced. This method can be effective when the dataset is large, and the majority class has a large number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cdcbaa",
   "metadata": {},
   "source": [
    "#### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f3e74",
   "metadata": {},
   "source": [
    "We can use Down Sampling to balance the dataset, some techniques of downsampling are:\n",
    "- Random under-sampling: Randomly removing samples from the majority class until the dataset is balanced. This method can be effective when the dataset is large and the majority class has a significant number of samples.\n",
    "- Stratified sampling: This method ensures that the resulting dataset has the same proportion of classes as the original dataset. It randomly selects samples from both the majority and minority classes, but it ensures that the ratio of samples in each class remains the same.\n",
    "- Cluster-based under-sampling: This method involves grouping samples in the majority class into clusters and then randomly selecting samples from each cluster until the dataset is balanced. This method can be effective when the majority class has a complex distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec93ef1",
   "metadata": {},
   "source": [
    "#### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd4589",
   "metadata": {},
   "source": [
    "In this scenario we can use the following methods of upsampling or over_sampling\n",
    "- Random over-sampling: Randomly duplicating samples from the minority class until the dataset is balanced. This method can be effective when the dataset is small and the minority class has a small number of samples.\n",
    "- SMOTE (Synthetic Minority Over-sampling Technique): This method generates new samples from the minority class by interpolating between the existing samples. It creates synthetic samples that are similar to the minority class samples but not identical. This method can be effective when the minority class has a small number of samples and there is a need to generate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6748f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
